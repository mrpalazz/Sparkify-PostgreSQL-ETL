# Sparkify Postgres ETL Pipeline using PostgreSQL
-Author, Michael Palazzolo
***

## Project Context:
Sparkify plans to set up for analyzing their user data related to song plays to derive insights about their behavior. 
The logs are in a series of JSON files that aren't immediately useful to the team, hence the need for an automated ETL pipeline.
Sparkify collects both usage data and user profile information in the JSON logs, the new data model will help the engineers in querying
for the conext they seek.

***

## ETL Pipeline Scope:
This is the first project from the Data Engineering nano-degree, the following outcome is expected
 * Generate a Data Model to represent the "Sparkify" song database.
 * Model the database using a star schema with fact and dimension tables.
 * Generate and ETL script in Python to handle the database.
 
 ***
 
## Star Schema Configuration:

A star schema is chosen for this application and will utilize a relational databaseto manage the queries, the following points justify this design choice.
The Sparkify team wants to undserstand more about the membership status of their users, including their song preference and behaviors on the app. This ETL pipeline will
enable them to dive into this with a structured dataset.

Why a relational database?
* The size of the data being used for the analysis is sufficiently small that it poses no challenges in this regard.
* The queries are flexible in a non-relational databse, so I can restructure them as needed as the project progresses.
* This configuration makes is easy for the data scientists or other business uses to query data from the structures.
* Joins can readily be performed on the tables generated in the ETL pipeline
 
### Fact Table:
The base data is in a JSON format.

* **songplays** - These are log records that show behavior of the users as they interact with the Sparkify application, records with page `nextsong` 
    * songplay_id **(Primary Key)**
    * start_time
    * user_id
    * level
    * song_id
    * artist_id
    * session_id
    * location
    * user_agent
    
### Dimension Tables:
The base data is in a JSON format.

* **users** - Sparkify app users 
    * user_id **(Primary Key)**
    * first_name
    * last_name
    * gender
    * level
    
    
* **songs** - songs in the music database
    * artist_id **(Primary Key)**
    * name
    * location
    * latitude
    * longitude
    
    
* **time** - the timestamps of the records `songplays` logs broken out into specific units 
    * artist_id **(Primary Key)**
    * name
    * location
    * latitude
    * longitude
 
***
## ETL pipeline creation and operation: 

1. `data` folder nested at the root directory of the project, where all json logs reside.
2. `sql_queries.py` All of the sql queries generated by the project requirements, the query is called by the other scripts when needed.
3. `create_tables.py` drops and creates tables. Run this file to reset your tables before each time you run your ETL scripts.
4. `test.ipynb` displays the first few rows of each table to let you check your database, good for checking that the process is working without going too far.
5. `etl.ipynb` reads and processes a single file from `song_data` and `log_data`, loads the data into the tables.
6. `etl.py` reads and processes files from `song_data` and `log_data` and loads them into the tables.
7. `README.md` currently viewed file, describes the ETL pipeline scope.

### Preliminary steps:

1. Write the `CREATE`, `DROP` and `INSERT` queries in `sql_queries.py`.
2. open a terminal and run ` python create_tables.py` to create database and tables.
3. Run `test.ipynb` to confirm the creation of tables with the correct columns along the way. Make sure to click "Restart kernel" to close the connection to the database after running this notebook.
4. set up the baseline data processing in the `ETL.ipynb` file, once they are generating as expected, transfer them over to the `ETL.py` script which runs the batch processing.

### Running the full pipeline:
1. complete the preliminary steps.
2. open a terminal and run ` python create_tables.py` to create database and tables.
3. in the terminal run `python etl.py`, this will kick off the ETL script and populate the data in the Sparkify database.
4. Run `test.ipynb` on to verify the talbes are populate as expected.
5. be sure the clear all kernels before trying to run the pipeline again.
 
 # Sparkify-PostgreSQL-ETL
